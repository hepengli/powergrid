# PowerGrid 2.0: Publication Roadmap

**Strategic Plan for Submission-Ready Research Paper**

**Document Version**: 1.0
**Date**: October 2025
**Target**: Top-tier venue acceptance by June 2026

---

## Executive Summary

This document outlines a **6-month execution plan** to transform PowerGrid 2.0 from a design proposal into a **publication-ready research contribution** acceptable at top AI/systems conferences.

**Key Insight**: Current proposal = engineering design document. Target venue = research paper with experiments, insights, and reproducible benchmarks.

---

## Current State Assessment

### What We Have âœ…
- Solid architectural design (agents, multi-agent env, hierarchical control)
- Clear vision for plug-and-play extensibility
- 3-month implementation roadmap
- Component diagrams and design documentation

### Critical Gaps âŒ
- **No working implementation** (only 3-month roadmap)
- **No experiments** (no baselines, no learning curves, no results)
- **No datasets** (CAISO/ERCOT mentioned but not integrated)
- **No novel research insights** (architectural proposals â‰  research findings)
- **No baseline comparisons** (need MPC, OPF, 5+ RL algorithms)
- **Wrong paper structure** (50 pages design vs. 15 pages experiments)

**Verdict**: Not submittable to any top-tier venue in current form.

---

## Lessons from Successful Papers

### Case Study 1: CityLearn (BuildSys 2019 â†’ NeurIPS Competition)

**Trajectory**:
1. **2019**: 2-page demo at BuildSys (systems venue, tool paper)
2. **2020**: Benchmarking study at RLEM workshop (experiments added)
3. **2020+**: NeurIPS official competition track (community adoption)

**Key Success Factors**:
- Started with **working implementation** at submission
- **Real datasets**: 9 buildings Ã— 4 climates Ã— 8760 hours = 315K data points
- **Baselines comparison**: RBC, SAC, PPO, QMIX, MADDPG (10 seeds each)
- **Novel insight**: "Independent learners competitive with complex MARL"
- **Systems venue first**, then ML venues

---

### Case Study 2: SustainGym (NeurIPS 2023 Datasets & Benchmarks)

**What They Did**:
- **5 diverse environments**: EV charging, HVAC, data centers, carbon-aware compute, battery
- **8 baselines**: SAC, TD3, PPO, A2C, IPPO, MAPPO, RBC, MPC
- **Rigorous evaluation**: 5 seeds, learning curves, ablations, distribution shift analysis
- **Novel insight**: "Off-the-shelf RL fails on sustainability (18-35% performance drop on unseen data)"
- **Paper structure**: 15 pages experiments, 4 pages env description, 2 pages intro

**Why It Succeeded**:
- Working package (pip install sustaingym)
- Comprehensive benchmarks with statistical rigor
- Identified clear research gap (generalization failure)
- 200+ training runs documented

---

## Target Venues Analysis

### Current Month: October 2025

| Venue | Submission Deadline | Conference Date | Fit | Recommendation |
|-------|-------------------|-----------------|-----|----------------|
| **ACM e-Energy 2026** (Winter) | **Jan 2026** | June 2026 | **Excellent** | âœ… **TARGET** |
| **ICML 2026** | TBA (likely Jan 2026) | July 2026 | Very High | âš ï¸ Backup |
| **NeurIPS 2026 D&B** | TBA (likely May 2026) | Dec 2026 | High | ðŸŽ¯ Stretch Goal |

---

### Recommended Primary Target: **ACM e-Energy 2026 (Winter Deadline)**

**Why This Venue**:
- âœ… **Systems-oriented**: Values practical tools + experimental validation
- âœ… **Energy domain**: Perfect fit for power grid applications
- âœ… **Realistic timeline**: 3 months to submission (Jan 2026)

**Conference Details**:
- **Location**: Banff, Canada
- **Dates**: June 22-25, 2026
- **Deadline**: January 2026 (Winter round)
- **Format**: 10 pages full paper (double-blind)

**Acceptance Criteria** (from CFP):
> "Papers that present novel systems, applications, algorithms, and experimental studies are welcome. Topics include but are limited to renewable energy integration, demand response, multi-agent control, and grid optimization."

**Perfect Fit**: Our contribution checks all boxes.

---

### Secondary Target: **NeurIPS 2026 Datasets & Benchmarks Track**

**Timeline**: If we complete experiments by March 2026, extend work for NeurIPS

**Why This Venue**:
- âœ… **Benchmark papers explicitly welcome** (SustainGym precedent)
- âœ… **Higher prestige** than e-Energy (top-tier ML conference)
- âœ… **Community impact**: NeurIPS audience = 10,000+ researchers

**Requirements** (higher bar than e-Energy):
- 5+ benchmark tasks (vs. 3 for e-Energy)
- 8+ baseline algorithms (vs. 5 for e-Energy)
- Distribution shift analysis (train/test split)
- Reproducibility package (Docker, detailed hyperparameters)

**Decision Point**: After e-Energy submission (Jan 2026), assess if we can extend for NeurIPS (May 2026 deadline, estimated)

---

## 6-Month Execution Plan

**Start Date**: October 2025
**End Date**: April 2026
**Primary Deliverable**: ACM e-Energy 2026 paper submission (Jan 2026)
**Stretch Deliverable**: Extended NeurIPS 2026 D&B submission (May 2026)

---

### Month 1 (Oct 2025): Foundation - Agent Abstraction

**Objective**: Complete core infrastructure for multi-agent RL

| Week | Owner | Task | Deliverable |
|------|-------|------|-------------|
| **Week 1** | Architect | Design `Agent` base class | `agents/base.py` with abstract methods |
| Week 1 | Architect | Implement `DeviceAgent` wrapper | Convert existing devices to agents |
| Week 1 | Domain | Refactor DG, ESS, RES to agents | 3 device types as `DeviceAgent` |
| Week 1 | DevOps | Set up pytest infrastructure | Test fixtures for agents |
| **Week 2** | Architect | Implement `GridCoordinatorAgent` | Coordinator for managing sub-agents |
| Week 2 | Domain | Refactor Shunt, Grid, Transformer | 3 more device types as agents |
| Week 2 | DevOps | Write unit tests for agents | >80% coverage on `agents/` |
| **Week 3** | Architect | Design action/observation spaces | Heterogeneous space composition |
| Week 3 | Domain | Test agent lifecycle (reset, act) | Integration tests |
| Week 3 | DevOps | CI/CD setup (GitHub Actions) | Auto-test on push |
| **Week 4** | All | Code review and refactor | Clean API, documentation |
| Week 4 | Architect | Merge agent abstraction PR | `agents/` module complete |

**Milestone**: âœ… All devices work as autonomous agents with clean API

**Deliverables**:
- `powergrid/agents/` module (5 files, 1000 LOC)
- Unit tests (>80% coverage)
- Integration tests (full lifecycle)
- API documentation

---

### Month 2 (Nov 2025): Multi-Agent Environment + Examples

**Objective**: PettingZoo-compatible environment with 3 working examples

| Week | Owner | Task | Deliverable |
|------|-------|------|-------------|
| **Week 5** | Architect | Implement `MultiAgentPowerGridEnv` | `envs/multi_agent/base.py` (PettingZoo API) |
| Week 5 | Architect | Action/observation space composition | Handle heterogeneous agents |
| Week 5 | Domain | Create IEEE 13-bus example (3 agents) | `envs/multi_agent/ieee13.py` |
| Week 5 | DevOps | RLlib integration test | Verify MAPPO trains |
| **Week 6** | Domain | Create IEEE 34-bus example (5 agents) | `envs/multi_agent/ieee34.py` |
| Week 6 | Domain | Create hierarchical example (2-level) | `envs/hierarchical/ieee34_2level.py` |
| Week 6 | DevOps | SB3 integration test | Verify PPO trains |
| **Week 7** | Architect | Implement reward aggregation | Support individual, shared, Shapley |
| Week 7 | Domain | YAML config for all 3 examples | `configs/*.yaml` |
| Week 7 | DevOps | Write integration tests | Test full env lifecycle |
| **Week 8** | All | Polish and document | README, examples, tutorials |
| Week 8 | Architect | Merge multi-agent env PR | `envs/multi_agent/` complete |

**Milestone**: âœ… Train MAPPO on IEEE 34-bus (5 agents), achieve convergence

**Deliverables**:
- PettingZoo environment (1 base class, 3 examples)
- 3 working environments (IEEE 13, IEEE 34, hierarchical)
- YAML configs for all examples
- Tutorial notebook: "Train Your First Multi-Agent Policy"

---

### Month 3 (Dec 2025): Datasets + Baseline Implementation

**Objective**: Real-world datasets + 5 baseline algorithms ready

| Week | Owner | Task | Deliverable |
|------|-------|------|-------------|
| **Week 9** | Domain | Download CAISO data (2020-2024) | 5 years load/solar/price (35K hours) |
| Week 9 | Domain | Preprocess datasets | Interpolate, normalize, train/test split |
| Week 9 | Domain | Build dataset loader | `datasets/loaders.py` with auto-download |
| Week 9 | DevOps | Verify data integrity | Checksums, missing data checks |
| **Week 10** | Architect | Integrate RLlib baselines | MAPPO, IPPO, SAC, PPO working |
| Week 10 | Domain | Implement MPC baseline (cvxpy) | Model predictive control benchmark |
| Week 10 | Domain | Implement OPF baseline (pandapower) | Optimal power flow benchmark |
| Week 10 | DevOps | Hyperparameter tuning scripts | Grid search for baselines |
| **Week 11** | Architect | Implement hierarchical coordinator | 2-level H-MAPPO algorithm |
| Week 11 | Domain | Define 5 benchmark tasks | Voltage reg, dispatch, peak shaving, resilience, scalability |
| Week 11 | DevOps | Training infrastructure (Ray cluster) | Parallel training setup |
| **Week 12** | All | Dry run all baselines on Task 1 | Verify convergence, debug issues |
| Week 12 | Architect | Document baseline APIs | How to add custom algorithms |

**Milestone**: âœ… All baselines run successfully on IEEE 34-bus voltage regulation task

**Deliverables**:
- CAISO dataset (5GB, auto-downloadable)
- 7 baseline algorithms: PPO, SAC, IPPO, MAPPO, H-MAPPO, MPC, OPF
- 5 benchmark task definitions
- Training scripts for all baselines

---

### Month 4 (Jan 2026): Experiments + Paper Drafting

**Objective**: Complete all experiments + first paper draft for e-Energy

| Week | Owner | Task | Deliverable |
|------|-------|------|-------------|
| **Week 13** | Architect | Run main experiments | 7 algos Ã— 5 tasks Ã— 5 seeds = **175 runs** |
| Week 13 | Domain | Monitor training, log metrics | TensorBoard logs, CSV exports |
| Week 13 | DevOps | Set up Ray cluster (4 GPUs) | Parallel training (25 jobs/day) |
| **Week 14** | Architect | Scalability study | 5, 10, 20, 50, 100 agents (time, memory) |
| Week 14 | Domain | Communication ablation | No comm, local, global, weighted |
| Week 14 | DevOps | Generate plots | Learning curves, bar charts, heatmaps |
| **Week 15** | Domain | Statistical analysis | t-tests, confidence intervals |
| Week 15 | DevOps | Create all figures/tables | Figures 1-8, Tables 1-3 (camera-ready) |
| Week 15 | Architect | Start paper draft (Intro, Related) | 4 pages written |
| **Week 16** | Architect | Write Methods section | 3 pages (env description, baselines) |
| Week 16 | Domain | Write Experiments section | 7 pages (results, analysis) |
| Week 16 | DevOps | Reproducibility package | Docker, scripts, README |
| **Week 17** | All | Internal review | Feedback from 3 reviewers |
| Week 17 | Architect | Revise based on feedback | Address comments |
| Week 17 | Domain | Proofread | Grammar, citations |
| **Week 18** | All | **Submit to e-Energy 2026** | 10-page paper + supplementary |

**Milestone**: ðŸŽ¯ **e-Energy 2026 Submission (Jan 2026)**

**Deliverables**:
- 175 training runs completed
- 8 figures (learning curves, scalability plots)
- 3 tables (performance, ablations, generalization)
- 10-page paper (ACM format)
- Reproducibility package (Docker, code, data)

---

### Month 5-6 (Feb-Apr 2026): Extension for NeurIPS (Optional)

**Objective**: Extend work for NeurIPS 2026 Datasets & Benchmarks Track

**Conditional on**: e-Energy reviews positive + team bandwidth available

| Task | Owner | Deliverable |
|------|-------|-------------|
| Add 3 more environments | Domain | IEEE 123, CIGRE MV, custom 200-bus |
| Add 2 more baselines | Architect | QMIX, MADDPG |
| Distribution shift analysis | Architect | Train on 2020-2022, test on 2023-2024 |
| GNN policy implementation | Architect | Graph neural network for topology generalization |
| Transfer learning study | Domain | Train on IEEE 34, test on IEEE 13/123 |
| Extended experiments | All | 10 algos Ã— 8 tasks Ã— 10 seeds = **800 runs** |
| Extended paper (20 pages) | All | Add 5 pages experiments, 2 pages theory |
| Submit to NeurIPS 2026 D&B | All | May 2026 (estimated deadline) |

**Milestone**: ðŸŽ¯ **NeurIPS 2026 D&B Submission (May 2026)**

---

## Paper Structure: e-Energy 2026

**Target Length**: 10 pages (ACM double-column format)

```markdown
PowerGrid: A Multi-Agent Reinforcement Learning Benchmark
for Scalable Hierarchical Control in Power Systems

1. Introduction (1.5 pages)
   - Problem: Power grids need distributed control for 100+ devices
   - Gap: No benchmark for hierarchical MARL at scale
   - Contribution: 5 tasks, 7 baselines, hierarchical coordination

2. Related Work (1 page)
   - Power systems RL: Grid2Op, CityLearn (buildings, not transmission)
   - MARL benchmarks: SMAC, MPE (not domain-specific)
   - Our positioning: Realistic physics + hierarchy + scale

3. PowerGrid Environment (2 pages)
   3.1 Core Design (0.5 pages)
       - PettingZoo API, pandapower backend
       - Agent abstraction, device plugins

   3.2 Benchmark Tasks (1.5 pages)
       Task 1: Voltage Regulation (IEEE 34, 5 agents)
       Task 2: Economic Dispatch (IEEE 123, 15 agents)
       Task 3: Peak Shaving (CIGRE MV, 8 agents)
       Task 4: Resilience (IEEE 34 + faults, 5 agents)
       Task 5: Scalability (custom 200-bus, 100 agents)

4. Baselines (1 page)
   - RL: PPO, SAC, IPPO, MAPPO, H-MAPPO
   - Classical: MPC, OPF
   - Hyperparameters (table)

5. Experiments (3.5 pages)
   5.1 Main Results (1 page)
       - Table 1: Performance on 5 tasks
       - Figure 1: Learning curves (5 plots)

   5.2 Scalability Analysis (1 page)
       - Figure 2: Training time vs # agents
       - Figure 3: Memory usage vs # agents
       - Key finding: Hierarchical scales to 100, centralized fails at 20

   5.3 Ablation Studies (1 page)
       - Communication: no/local/global/weighted
       - Hierarchy depth: flat/2-level/3-level
       - Figure 4: Ablation results

   5.4 Discussion (0.5 pages)
       - When does hierarchy help? (N>20, bandwidth limited)
       - Failure modes, limitations

6. Conclusion (0.5 pages)
   - Summary of contributions
   - Future work: hardware-in-loop, three-phase

7. References (0.5 pages)
   - 30-40 citations

Supplementary Material (no page limit)
   - Full hyperparameters
   - Additional plots
   - Reproducibility checklist
```

---

## Key Research Insights (To Be Discovered)

**These are hypotheses to test during experiments**:

### Hypothesis 1: Scalability
> "Hierarchical MARL scales to 100+ agents while centralized methods fail beyond 20 agents due to exponential action space growth."

**Evidence needed**:
- Plot: Training time vs # agents (centralized exponential, hierarchical linear)
- Plot: Final performance vs # agents (centralized plateaus/degrades, hierarchical maintains)
- Table: Memory usage (centralized OOM at N=25, hierarchical works at N=100)

---

### Hypothesis 2: Communication Efficiency
> "Voltage-weighted communication reduces bandwidth by 90% while retaining 95% of performance compared to full communication."

**Evidence needed**:
- Ablation: no comm (70% performance), local (85%), global (100%), weighted (95%)
- Plot: Performance vs communication budget (show Pareto frontier)
- Table: Messages sent per step (global=NÂ², weighted=N/10)

---

### Hypothesis 3: Hierarchy Depth
> "Two-level hierarchy is optimal: flat fails to coordinate, three-level adds overhead without benefit."

**Evidence needed**:
- Comparison: flat (fails on 50+ agents), 2-level (best performance), 3-level (slower convergence)
- Figure: Convergence speed vs hierarchy depth (inverted U-shape)

---

## Success Metrics

### Publication Acceptance (Primary Goal)
- âœ… **e-Energy 2026 acceptance** (target: 25-30% acceptance rate)
- ðŸŽ¯ **NeurIPS 2026 D&B acceptance** (stretch: 20-25% acceptance rate)

### Technical Metrics
- âœ… 175+ training runs completed (5 seeds per config)
- âœ… 5 benchmark tasks defined and evaluated
- âœ… 7+ baseline algorithms implemented
- âœ… Statistical significance (p<0.05 on key findings)
- âœ… Reproducibility package (Docker, one-command setup)

### Community Impact (12-month post-publication)
- ðŸŽ¯ 50+ GitHub stars
- ðŸŽ¯ 5+ external users (issue reports, PRs)
- ðŸŽ¯ 3+ papers citing our work
- ðŸŽ¯ 1+ follow-up publications using benchmark

---

## Resource Requirements

### Compute
| Resource | Quantity | Duration | Cost |
|----------|----------|----------|------|
| GPU (A100) | 4 nodes | 6 weeks | $2,000 (cloud credits) |
| CPU (32 cores) | 1 node | 3 months | $500 |
| Storage | 500 GB | 6 months | $100 |
| **Total** | | | **$2,600** |

**Optimization**: Use academic cloud credits (AWS, GCP, Azure for Research)

---

### Data
| Dataset | Source | Size | License |
|---------|--------|------|---------|
| CAISO load/solar/price | oasis.caiso.com | 5 GB | Public |
| IEEE test networks | pandapower | <100 MB | Open |
| NREL solar profiles | NSRDB | 2 GB | Open |

**Total download**: ~7 GB (auto-downloadable via scripts)

---

## Risk Mitigation

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| **Experiments don't converge** | Medium | High | Start experiments in Week 12 (dry run), debug early |
| **CAISO data access issues** | Low | Medium | Use synthetic data as fallback (load_profiles library) |
| **Baselines crash on 100 agents** | Medium | High | Reduce to 50 agents for centralized, highlight as limitation |
| **e-Energy deadline miss** | Low | Critical | Weekly checkpoints, buffer 2 weeks before deadline |
| **GPU availability** | Medium | High | Reserve cloud credits early, use preemptible instances |
| **Scope creep** | High | High | **Strict feature freeze after Month 2** |

### Contingency Plan

**If behind schedule by end of Month 3**:

**Priority 1 (Must Have for e-Energy)**:
- âœ… 3 baseline algorithms (PPO, MAPPO, MPC)
- âœ… 3 benchmark tasks (voltage, dispatch, scalability)
- âœ… Main results table + 3 learning curves

**Priority 2 (Should Have)**:
- âš ï¸ 5 baselines (defer OPF, SAC if needed)
- âš ï¸ Ablation studies (communication only, skip hierarchy depth)

**Priority 3 (Nice to Have - Defer to NeurIPS)**:
- âŒ Distribution shift analysis
- âŒ Transfer learning study
- âŒ GNN policies

---

## Weekly Milestones & Checkpoints

**Every Monday**: Sprint planning (2 hours)
- Review previous week's deliverables
- Assign tasks for current week
- Identify blockers

**Every Wednesday**: Mid-week sync (1 hour)
- Progress check
- Resolve blockers
- Adjust timeline if needed

**Every Friday**: Demo + retrospective (2 hours)
- Demo working features
- Discuss what went well/poorly
- Update roadmap for next week

**Monthly Reviews** (Last Friday of month):
- âœ… Month 1: Agent abstraction complete?
- âœ… Month 2: Multi-agent env working?
- âœ… Month 3: Baselines + datasets ready?
- ðŸŽ¯ Month 4: Paper submitted?

---

## Expected Reviewer Feedback & Responses

### After e-Energy Submission (Feb 2026)

**Likely Positive Comments**:
> "Well-designed benchmark with comprehensive evaluation. The scalability study is valuable for the community."

> "Real-world datasets and classical baselines (MPC, OPF) make this practical. Reproducibility package is excellent."

**Likely Critical Comments**:
> "Why only voltage regulation? Real grids have frequency control, protection, etc."

**Response Strategy**:
- Acknowledge limitation in discussion section
- Argue: voltage/dispatch are primary use cases for RL (frequency = milliseconds, out of scope)
- Promise future work on dynamic control

---

> "Hierarchical method is just parameter sharing. What's novel?"

**Response Strategy**:
- Emphasize: novel contribution is benchmark + baselines, not algorithm
- H-MAPPO is ablation to show hierarchy helps, not claimed as novel algorithm
- If reviewer insists: add voltage-weighted communication as novel coordination mechanism

---

> "Only 5 seeds? Standard is 10+ for RL."

**Response Strategy**:
- Acknowledge, run 5 more seeds in rebuttal period (2 weeks)
- Update figures with 10 seeds
- Cite computational cost (800 GPU-hours already)

---

## Success Stories: What Acceptance Looks Like

### e-Energy 2026 Acceptance (March 2026)
**Expected Decision**: Accept (conditional minor revisions)

**Revisions Required**:
- Add 5 more seeds per experiment (increase from 5 to 10)
- Clarify MPC baseline setup (which cost function?)
- Add comparison with existing tools (Grid2Op, CityLearn)
- Expand discussion of limitations

**Timeline**: 2 weeks for revisions, camera-ready by mid-April

**Presentation**: Oral presentation at e-Energy (June 2026, Banff)

---

### NeurIPS 2026 D&B Acceptance (August 2026)
**Expected Decision**: Accept (after rebuttal)

**Rebuttal Points**:
- Address scalability concerns (show 100 agents work)
- Add distribution shift analysis (requested by Reviewer 2)
- Clarify generalization experiments (requested by Reviewer 3)

**Timeline**: Camera-ready by September, poster at NeurIPS (December 2026)

**Impact**:
- Featured in NeurIPS benchmarks track
- Invited to present at Climate Change AI workshop
- Potential for tutorial/challenge in 2027

---

## Long-Term Vision (12-18 Months)

### Phase 1: Publication (Months 1-6)
- âœ… e-Energy 2026 accepted
- âœ… Open-source release (pip install powergrid)
- âœ… Documentation site launched

### Phase 2: Adoption (Months 7-12)
- ðŸŽ¯ 50+ GitHub stars
- ðŸŽ¯ 5+ external users
- ðŸŽ¯ NeurIPS 2026 D&B accepted

### Phase 3: Ecosystem (Months 13-18)
- ðŸŽ¯ 10+ community-contributed devices
- ðŸŽ¯ 5+ research papers using benchmark
- ðŸŽ¯ Tutorial at AAAI/IJCAI

---

## Appendix A: Detailed Timeline (Gantt Chart)

```
Month 1 (Oct 2025): Agent Abstraction
â”œâ”€â”€ Week 1: Base classes
â”œâ”€â”€ Week 2: Device refactor
â”œâ”€â”€ Week 3: Testing
â””â”€â”€ Week 4: Review & merge

Month 2 (Nov 2025): Multi-Agent Environment
â”œâ”€â”€ Week 5: PettingZoo env
â”œâ”€â”€ Week 6: Examples (IEEE 13/34)
â”œâ”€â”€ Week 7: Hierarchical example
â””â”€â”€ Week 8: Documentation

Month 3 (Dec 2025): Datasets + Baselines
â”œâ”€â”€ Week 9: CAISO data download
â”œâ”€â”€ Week 10: RLlib integration
â”œâ”€â”€ Week 11: MPC/OPF baselines
â””â”€â”€ Week 12: Dry run experiments

Month 4 (Jan 2026): Experiments + Paper
â”œâ”€â”€ Week 13-14: Main experiments (175 runs)
â”œâ”€â”€ Week 15: Ablations + plots
â”œâ”€â”€ Week 16: Paper draft
â”œâ”€â”€ Week 17: Internal review
â””â”€â”€ Week 18: Submit to e-Energy ðŸŽ¯

Month 5-6 (Feb-Apr 2026): NeurIPS Extension (Optional)
â”œâ”€â”€ Add 3 environments
â”œâ”€â”€ Add 2 baselines
â”œâ”€â”€ 800 training runs
â”œâ”€â”€ Extended paper (20 pages)
â””â”€â”€ Submit to NeurIPS D&B ðŸŽ¯
```

---
